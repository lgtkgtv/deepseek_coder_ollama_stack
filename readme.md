# ğŸ“– README.md (DeepSeek Coder 1.3B - Docker-Only, CPU-Only)

```markdown
# DeepSeek Coder 1.3B - Docker-only Stack (CPU-only)

This stack provides a **fully Docker-based deployment** of:
- Ollama (for serving DeepSeek Coder 1.3B on CPU)
- Open WebUI (a user-friendly chat interface)

> ğŸš¨ This setup **requires Docker** (NOT Podman).  
> It is also tailored for **CPU-only systems** â€” no GPU needed.

---

## ğŸ“¦ System Requirements

âœ… Ubuntu 22.04 **or newer**  
âœ… Docker 24+ (installed correctly)  
âœ… CPU with **AVX2 support** (run: `grep avx2 /proc/cpuinfo`)  
âœ… At least **8GB RAM** (more is better)  
âœ… Internet connection (to pull the model)

---

## âš™ï¸ Quick Setup Instructions

### 1ï¸âƒ£ Install Docker & Docker Compose

If Docker is not installed, run:
```bash
sudo apt-get update
sudo apt-get install docker.io docker-compose
```

Confirm correct versions:
```bash
docker --version
docker-compose --version
```

---

### 2ï¸âƒ£ Clone This Repository

```bash
https://github.com/lgtkgtv/deepseek_coder_ollama_stack.git
cd deepseek_coder_ollama_stack
```

---

### 3ï¸âƒ£ Start Ollama + Open WebUI

```bash
# docker compose up -d
docker-compose up -d
```

Check they are running:
```bash
docker compose ps
```

---

### 4ï¸âƒ£ Pull the Model into Ollama

This step downloads **DeepSeek Coder 1.3B** into your local storage:
```bash
./pull_model.sh
```

> This may take a few minutes (depending on your internet speed).

---

### 5ï¸âƒ£ Open the WebUI

Visit:  
ğŸ‘‰ [http://localhost:3000](http://localhost:3000)  
You can now chat with DeepSeek Coder 1.3B directly.

---

## âœ… CPU Optimization (Optional)

Ollama automatically detects CPU-only hardware. No extra configuration is needed.

If you want to explicitly disable GPU usage (for safety), you can add to `docker-compose.yml`:
```yaml
    environment:
      OLLAMA_DISABLE_CUDA: "1"
```
This is not required for CPU-only machines, but can add clarity.

---

## ğŸš¨ Important Security Notes

âœ… Docker Compose explicitly binds ports to **localhost only** (127.0.0.1)  
âœ… Your data (models + chat history) stays inside `ollama_data` and `openwebui_data`  
âœ… No external network exposure unless you modify `docker-compose.yml`

---

## ğŸ›‘ Stopping the Stack

```bash
docker compose down
```

---

## ğŸ”„ Updating Open WebUI (if needed)

```bash
docker compose pull openwebui
docker compose up -d
```

---

## ğŸ“‚ Folder Structure

```
.
â”œâ”€â”€ docker-compose.yml    # Full stack definition (Ollama + Open WebUI)
â”œâ”€â”€ pull_model.sh         # Downloads DeepSeek Coder 1.3B into Ollama
â”œâ”€â”€ ollama_data/           # Ollama models are stored here
â””â”€â”€ openwebui_data/        # WebUI history & settings are stored here
```

---

## ğŸ’¬ Need Help?

If something breaks, check logs:
```bash
docker compose logs
```

If Ollama is down, itâ€™s usually a hardware compatibility issue. Check CPU with:
```bash
grep avx2 /proc/cpuinfo
```

---

## âŒ Podman Warning

This stack **will NOT work with Podman**. Ensure Podman is NOT installed:
```bash
sudo apt-get remove --purge podman podman-compose
```
Check `docker-compose version` to ensure itâ€™s the **real Docker Compose**:
```bash
docker-compose version
```

---

Happy coding! ğŸš€ğŸ“¥ ğŸ˜ƒ
```
---
