version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    # No --gpus=all here as you don't have a GPU
    ports:
      - "11434:11434" # Ollama API port
    volumes:
      - ollama_models:/root/.ollama # Persistent storage for models
    restart: unless-stopped
    # Environment variables for CPU optimization (optional, can be added if you face issues)
    # environment:
    #   - OLLAMA_HOST=0.0.0.0
    #   - OLLAMA_NUM_PARALLEL=1 # Limit parallel requests for CPU
    #   - OLLAMA_MAX_LOADED_MODELS=1 # Limit loaded models for CPU

  open-webui:
    image: ghcr.io/open-webui/open-webui:ollama
    container_name: open-webui
    ports:
      - "3000:8080" # WebUI access port (Host:3000 -> Container:8080)
    volumes:
      - open-webui_data:/app/backend/data # Persistent storage for WebUI data (chat history, users)
    depends_on:
      - ollama # Ensures Ollama starts before Open WebUI
    environment:
      # Optional: Set a secret key for Open WebUI for added security (replace with a strong, random string)
      # - WEBUI_SECRET_KEY=your_super_secret_key_here
      - OLLAMA_BASE_URL=http://ollama:11434 # Point to the Ollama service within the Docker network
    restart: unless-stopped

volumes:
  ollama_models:
  open-webui_data:
